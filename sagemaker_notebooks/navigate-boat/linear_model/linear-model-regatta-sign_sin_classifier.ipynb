{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42b5e80b-ad1d-4335-a1f7-10a91127e3dc"
    }
   },
   "source": [
    "# Heading prediction \n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "1. [Predict](#Predict)\n",
    "1. [Extensions](#Extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "This notebook illustrates how one can use SageMaker's algorithms for solving applications which require `linear models` for prediction.\n",
    "* Basic setup for using SageMaker.\n",
    "* converting datasets to protobuf format used by the Amazon SageMaker algorithms and uploading to S3. \n",
    "* Training SageMaker's linear learner on the data set.\n",
    "* Hosting the trained model.\n",
    "* Scoring using the trained model.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The SageMaker role arn used to give learning and hosting access to your data. The snippet below will use the same role used by your SageMaker notebook instance, if you're using other.  Otherwise, specify the full ARN of a role with the SageMakerFullAccess policy attached.\n",
    "* The S3 bucket that you want to use for training and storing model objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b2548d66-6f8f-426f-9cda-7a3cd1459abd"
    }
   },
   "source": [
    "Now we'll import the Python libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "nbpresent": {
     "id": "bb88eea9-27f3-4e47-9133-663911ea09a9"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import sagemaker.amazon.common as smac\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from os.path import join\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "isConfigCell": true,
    "nbpresent": {
     "id": "6427e831-8f89-45c0-b150-0b134397d79a"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "bucket = 'virtual-regatta-ml'\n",
    "\n",
    "prefix = 'linear-learner' # place to upload training files within the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "142777ae-c072-448e-b941-72bc75735d01"
    }
   },
   "source": [
    "---\n",
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boat_speed</th>\n",
       "      <th>angle_of_attack</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>cos_target_angle</th>\n",
       "      <th>sign_sin_target_angle</th>\n",
       "      <th>sign_sin_boat_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5776.000000</td>\n",
       "      <td>5776.000000</td>\n",
       "      <td>5776.000000</td>\n",
       "      <td>5776.000000</td>\n",
       "      <td>5776.000000</td>\n",
       "      <td>5776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.482229</td>\n",
       "      <td>23.110922</td>\n",
       "      <td>7.977421</td>\n",
       "      <td>-0.284007</td>\n",
       "      <td>0.504155</td>\n",
       "      <td>0.538954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.285287</td>\n",
       "      <td>107.029085</td>\n",
       "      <td>5.005491</td>\n",
       "      <td>0.704256</td>\n",
       "      <td>0.500026</td>\n",
       "      <td>0.498523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-177.061360</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.788728</td>\n",
       "      <td>-79.426525</td>\n",
       "      <td>4.476337</td>\n",
       "      <td>-0.970798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.883440</td>\n",
       "      <td>45.210510</td>\n",
       "      <td>6.260880</td>\n",
       "      <td>-0.348948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.575180</td>\n",
       "      <td>120.204100</td>\n",
       "      <td>9.852773</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.954010</td>\n",
       "      <td>173.899040</td>\n",
       "      <td>21.077130</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        boat_speed  angle_of_attack   wind_speed  cos_target_angle  \\\n",
       "count  5776.000000      5776.000000  5776.000000       5776.000000   \n",
       "mean      5.482229        23.110922     7.977421         -0.284007   \n",
       "std       2.285287       107.029085     5.005491          0.704256   \n",
       "min       0.000000      -177.061360     2.000000         -1.000000   \n",
       "25%       3.788728       -79.426525     4.476337         -0.970798   \n",
       "50%       4.883440        45.210510     6.260880         -0.348948   \n",
       "75%       7.575180       120.204100     9.852773          0.192382   \n",
       "max       9.954010       173.899040    21.077130          0.999996   \n",
       "\n",
       "       sign_sin_target_angle  sign_sin_boat_angle  \n",
       "count            5776.000000          5776.000000  \n",
       "mean                0.504155             0.538954  \n",
       "std                 0.500026             0.498523  \n",
       "min                 0.000000             0.000000  \n",
       "25%                 0.000000             0.000000  \n",
       "50%                 1.000000             1.000000  \n",
       "75%                 1.000000             1.000000  \n",
       "max                 1.000000             1.000000  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../all-in-one.csv')\n",
    "\n",
    "# This column will be predicted using regression\n",
    "data = data.drop(columns=['cos_boat_angle'])\n",
    "\n",
    "# The column to predict has to be in last position\n",
    "df_reordered = data[['boat_speed','angle_of_attack','wind_speed','cos_target_angle','sign_sin_target_angle', 'sign_sin_boat_angle']]\n",
    "\n",
    "# Binary classifier only accepts 0 and 1\n",
    "df_reordered['sign_sin_target_angle'] = df_reordered['sign_sin_target_angle'].map({-1:0, 1:1})\n",
    "df_reordered['sign_sin_boat_angle'] = df_reordered['sign_sin_boat_angle'].map({-1:0, 1:1})\n",
    "\n",
    "df_reordered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features and Labels\n",
    "#### Split the data into 80% training, 10% validation and 10% testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ff9d10f9-b611-423b-80da-6dcdafd1c8b9"
    }
   },
   "source": [
    "Now, we'll convert the datasets to the recordIO-wrapped protobuf format used by the Amazon SageMaker algorithms, and then upload this data to S3.  We'll start with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_and_validation_data_to_s3(bucket, test_name, data) :\n",
    "    rand_split = np.random.rand(len(data))\n",
    "    train_list = rand_split < 0.8\n",
    "    val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "    test_list = rand_split >= 0.9\n",
    "\n",
    "    data_train = data[train_list]\n",
    "    data_val = data[val_list]\n",
    "    data_test = data[test_list]\n",
    "\n",
    "    train_y = data_train.iloc[:,-1].to_numpy();\n",
    "    train_X = data_train.iloc[:,:-1].to_numpy();\n",
    "    print(train_X)\n",
    "    print(train_y)\n",
    "\n",
    "    val_y = data_val.iloc[:,-1].to_numpy();\n",
    "    val_X = data_val.iloc[:,:-1].to_numpy();\n",
    "\n",
    "    test_y = data_test.iloc[:,-1].to_numpy();\n",
    "    test_X = data_test.iloc[:,:-1].to_numpy();\n",
    "    train_file = 'linear_train.data'\n",
    "\n",
    "    f = io.BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\n",
    "    f.seek(0)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, test_name, 'train', train_file)).upload_fileobj(f)\n",
    "    validation_file = 'linear_validation.data'\n",
    "    \n",
    "    f = io.BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'), val_y.astype('float32'))\n",
    "    f.seek(0)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, test_name,'validation', validation_file)).upload_fileobj(f)\n",
    "    return (test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   7.73236    -123.9447        9.43467      -0.48980636    0.        ]\n",
      " [   7.81913    -124.9447        9.66396      -0.51548187    0.        ]\n",
      " [   7.84463    -124.9447        9.72758      -0.5556533     0.        ]\n",
      " ...\n",
      " [   4.01797    -134.9213        4.41181      -0.49292428    0.        ]\n",
      " [   4.1575     -134.9213        4.5656       -0.46829154    0.        ]\n",
      " [   4.24893    -134.9213        4.66638      -0.45540256    0.        ]]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "test_name = 'default'\n",
    "\n",
    "test_X, test_y = save_train_and_validation_data_to_s3(bucket, test_name, df_reordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f3b125ad-a2d5-464c-8cfa-bd203034eee4"
    }
   },
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Now we can begin to specify our linear model.  Amazon SageMaker's Linear Learner actually fits many models in parallel, each with slightly different hyperparameters, and then returns the one with the best fit.  This functionality is automatically enabled.  We can influence this using parameters like:\n",
    "\n",
    "- `num_models` to increase to total number of models run.  The specified parameters will always be one of those models, but the algorithm also chooses models with nearby parameter values in order to find a solution nearby that may be more optimal.  In this case, we're going to use the max of 32.\n",
    "- `loss` which controls how we penalize mistakes in our model estimates.  For this case, let's use absolute loss as we haven't spent much time cleaning the data, and absolute loss will be less sensitive to outliers.\n",
    "- `wd` or `l1` which control regularization.  Regularization can prevent model overfitting by preventing our estimates from becoming too finely tuned to the training data, which can actually hurt generalizability.  In this case, we'll leave these parameters as their default \"auto\" though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_training(sm, container, bucket, prefix, test_number, instance_type, hyperparameters):\n",
    "\n",
    "    linear_job = 'VR-sign-sin-classifier-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "    print(\"Job name is:\", linear_job)\n",
    "\n",
    "    linear_training_params = {\n",
    "        \"RoleArn\": role,\n",
    "        \"TrainingJobName\": linear_job,\n",
    "        \"AlgorithmSpecification\": {\n",
    "            \"TrainingImage\": container,\n",
    "            \"TrainingInputMode\": \"File\"\n",
    "        },\n",
    "        \"ResourceConfig\": {\n",
    "            \"InstanceCount\": 1,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"VolumeSizeInGB\": 1\n",
    "        },\n",
    "        \"InputDataConfig\": [\n",
    "            {\n",
    "                \"ChannelName\": \"train\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": \"s3://{}/{}/{}/train/\".format(bucket, prefix, test_number),\n",
    "                        \"S3DataDistributionType\": \"ShardedByS3Key\"\n",
    "                    }\n",
    "                },\n",
    "                \"CompressionType\": \"None\",\n",
    "                \"RecordWrapperType\": \"None\"\n",
    "            },\n",
    "            {\n",
    "                \"ChannelName\": \"validation\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": \"s3://{}/{}/{}/validation/\".format(bucket, prefix, test_number),\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                    }\n",
    "                },\n",
    "                \"CompressionType\": \"None\",\n",
    "                \"RecordWrapperType\": \"None\"\n",
    "            }\n",
    "\n",
    "        ],\n",
    "        \"OutputDataConfig\": {\n",
    "            \"S3OutputPath\": \"s3://{}/{}/{}/\".format(bucket, prefix, test_number)\n",
    "        },\n",
    "        \"HyperParameters\": hyperparameters,\n",
    "        \"StoppingCondition\": {\n",
    "            \"MaxRuntimeInSeconds\": 60 * 60\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sm.create_training_job(**linear_training_params)\n",
    "\n",
    "    status = sm.describe_training_job(TrainingJobName=linear_job)['TrainingJobStatus']\n",
    "    print(status)\n",
    "    sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=linear_job)\n",
    "    print(\"training done\")\n",
    "    return linear_job\n",
    "    if status == 'Failed':\n",
    "        message = sm.describe_training_job(TrainingJobName=linear_job)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify container images used for training and hosting SageMaker's linear-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# See 'Algorithms Provided by Amazon SageMaker: Common Parameters' in the SageMaker documentation for an explanation of these values.\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's kick off our training job in SageMaker's distributed, managed training, using the parameters we just created.  Because training is managed, we don't have to wait for our job to finish to continue, but for this case, let's use boto3's 'training_job_completed_or_stopped' waiter so we can ensure that the job has been started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name is: VR-sign-sin-classifier-2020-09-24-13-23-30\n",
      "InProgress\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "sm = boto3.client('sagemaker')\n",
    "hyperparameters = {\n",
    "        \"feature_dim\": \"5\",\n",
    "        \"mini_batch_size\": \"64\",\n",
    "        \"predictor_type\": \"binary_classifier\",\n",
    "        \"epochs\": \"100\",\n",
    "        \"num_models\": \"32\",\n",
    "        \"loss\": \"logistic\"\n",
    "    }\n",
    "instance_type = \"ml.m5.large\"\n",
    "\n",
    "linear_job = launch_training(sm, container, bucket, prefix, test_name, instance_type, hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2adcc348-9ab5-4a8a-8139-d0ecd740208a"
    }
   },
   "source": [
    "---\n",
    "## Host\n",
    "\n",
    "Now that we've trained the linear algorithm on our data, let's setup a model which can later be hosted.  We will:\n",
    "1. Point to the scoring container\n",
    "1. Point to the model.tar.gz that came from training\n",
    "1. Create the hosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've setup a model, we can configure what our hosting endpoints should be.  Here we specify:\n",
    "1. EC2 instance type to use for hosting\n",
    "1. Initial number of instances\n",
    "1. Our hosting model name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've specified how our endpoint should be configured, we can create them.  This can be done in the background, but for now let's run a loop that updates us on the status of the endpoints so that we know when they are ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_endpoint(container, linear_job, role, instance_type) :\n",
    "\n",
    "    linear_hosting_container = {\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': sm.describe_training_job(TrainingJobName=linear_job)['ModelArtifacts']['S3ModelArtifacts']\n",
    "    }\n",
    "\n",
    "    create_model_response = sm.create_model(\n",
    "    ModelName=linear_job,\n",
    "        ExecutionRoleArn=role,\n",
    "        PrimaryContainer=linear_hosting_container)\n",
    "\n",
    "    print(create_model_response['ModelArn'])\n",
    "\n",
    "    linear_endpoint_config = 'VR-sign-sin-classifier-endpoint-config-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "    print(linear_endpoint_config)\n",
    "    create_endpoint_config_response = sm.create_endpoint_config(\n",
    "        EndpointConfigName=linear_endpoint_config,\n",
    "        ProductionVariants=[{\n",
    "            'InstanceType': instance_type,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'ModelName': linear_job,\n",
    "            'VariantName': 'AllTraffic'}])\n",
    "\n",
    "    print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n",
    "\n",
    "    linear_endpoint = 'VR-sign-sin-classifier-endpoint-' + time.strftime(\"%Y%m%d%H%M\", time.gmtime())\n",
    "    print(linear_endpoint)\n",
    "    create_endpoint_response = sm.create_endpoint(\n",
    "        EndpointName=linear_endpoint,\n",
    "        EndpointConfigName=linear_endpoint_config)\n",
    "    print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "    resp = sm.describe_endpoint(EndpointName=linear_endpoint)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "    sm.get_waiter('endpoint_in_service').wait(EndpointName=linear_endpoint)\n",
    "\n",
    "    resp = sm.describe_endpoint(EndpointName=linear_endpoint)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Status: \" + status)\n",
    "    return linear_endpoint\n",
    "    if status != 'InService':\n",
    "        raise Exception('Endpoint creation did not succeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:eu-west-2:281633979087:model/vr-sign-sin-classifier-2020-09-24-13-23-30\n",
      "VR-sign-sin-classifier-endpoint-config-2020-09-24-13-31-31\n",
      "Endpoint Config Arn: arn:aws:sagemaker:eu-west-2:281633979087:endpoint-config/vr-sign-sin-classifier-endpoint-config-2020-09-24-13-31-31\n",
      "VR-sign-sin-classifier-endpoint-202009241331\n",
      "arn:aws:sagemaker:eu-west-2:281633979087:endpoint/vr-sign-sin-classifier-endpoint-202009241331\n",
      "Status: Creating\n",
      "Arn: arn:aws:sagemaker:eu-west-2:281633979087:endpoint/vr-sign-sin-classifier-endpoint-202009241331\n",
      "Status: InService\n",
      "CPU times: user 229 ms, sys: 19.1 ms, total: 248 ms\n",
      "Wall time: 7min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "instance_type = 'ml.t2.medium'\n",
    "\n",
    "linear_endpoint = launch_endpoint(container, linear_job, role, instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "### Predict on Test Data\n",
    "\n",
    "Now that we have our hosted endpoint, we can generate statistical predictions from it.  Let's predict on our test dataset to understand how accurate our model is.\n",
    "\n",
    "There are many metrics to measure classification accuracy.  Common examples include include:\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 measure\n",
    "- Area under the ROC curve - AUC\n",
    "- Total Classification Accuracy \n",
    "- Mean Absolute Error\n",
    "\n",
    "For our example, we'll keep things simple and use total classification accuracy as our metric of choice. We will also evaluate  Mean Absolute  Error (MAE) as the linear-learner has been optimized using this metric, not necessarily because it is a relevant metric from an application point of view. We'll compare the performance of the linear-learner against a naive benchmark prediction which uses majority class observed in the training data set for prediction on the test data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert an array to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    return csv.getvalue().decode().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll invoke the endpoint to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "payload = np2csv(test_X)\n",
    "response = runtime.invoke_endpoint(EndpointName=linear_endpoint,\n",
    "                                   ContentType='text/csv',\n",
    "                                   Body=payload)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "test_pred = np.array([r['score'] for r in result['predictions']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare linear learner based mean absolute prediction errors from a baseline prediction which uses majority class to predict every instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE Linear: 0.291\n"
     ]
    }
   ],
   "source": [
    "test_mae_linear = np.mean(np.abs(test_y - test_pred))\n",
    "\n",
    "print(\"Test MAE Linear:\", round(test_mae_linear,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(test_y, test_pred)\n",
    "print(\"Test MSE:\", round(mse,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extensions\n",
    "\n",
    "- Our linear model does a good job of predicting breast cancer and has an overall accuracy of close to 92%. We can re-run the model with different values of the hyper-parameters, loss functions etc and see if we get improved prediction. Re-running the model with further tweaks to these hyperparameters may provide more accurate out-of-sample predictions.\n",
    "- We also did not do much feature engineering. We can create additional features by considering cross-product/intreaction of multiple features, squaring or raising higher powers of the features to induce non-linear effects, etc. If we expand the features using non-linear terms and interactions, we can then tweak the regulaization parameter to optimize the expanded model and hence generate improved forecasts.\n",
    "- As a further extension, we can use many of non-linear models available through SageMaker such as XGBoost, MXNet etc.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the License). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the license file accompanying this file. This file is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
